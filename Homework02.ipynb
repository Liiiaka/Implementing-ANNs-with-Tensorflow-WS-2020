{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implements the sigmoid, which we use as activation function, and sigmoidprime(its derivative) function\n",
    "def sigmoid(input):\n",
    "    return 1/(1+np.exp(-input))\n",
    "\n",
    "def sigmoidprime(input):\n",
    "    return sigmoid(input)*(1-sigmoid(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the four possible input pairs of (x1,x2).\n",
    "possible_inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "# These are possible labels form some logical gates.\n",
    "t_and = np.array([0,0,0,1])\n",
    "t_or = np.array([0,1,1,1])\n",
    "t_nand = np.array([1,1,1,0])\n",
    "t_nor = np.array([1,0,0,0])\n",
    "t_xor = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron class from the flipped classroom session, changed alpha to 1, the activation funct in 'forward_step' to sigmoid\n",
    "# and replaced the funct 'training_step' with 'update'\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_units):\n",
    "        self.input_units = input_units\n",
    "        # 1. Initialize random weights and a random bias term. Check 'np.random.randn()'.\n",
    "        self.weights = np.random.randn(input_units)\n",
    "        self.bias = np.random.randn()\n",
    "        # 2. Define the learning rate as 0.01.\n",
    "        self.alpha = 1\n",
    "        self.drive = 0\n",
    "        self.inputs = np.array(range(input_units))\n",
    "        \n",
    "    def forward_step(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Perform a perceptron forward step.\n",
    "        # 1. Calculate the drive\n",
    "        self.drive =  self.weights @ inputs + self.bias \n",
    "\n",
    "        # return the value of the activation function, which is sigmoid\n",
    "        return sigmoid(self.drive)\n",
    "    \n",
    "    '''\n",
    "    delta: np.array([]) <- Takes the error value for the respective perceptron.\n",
    "    Uses delta to calculate the lambdas for bias and weights and uses those lambdas to update bias and weights.\n",
    "    '''\n",
    "    def update(self, delta):\n",
    "        lambda_bias = delta * 1\n",
    "        lambda_weights = delta * self.inputs\n",
    "        self.bias = self.bias - self.alpha * lambda_bias\n",
    "        self.weights = self.weights - self.alpha * lambda_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp class\n",
    "class MLP:\n",
    "    '''\n",
    "    network_structure = np.array([]) <- Takes in the structure of the network. The first index is the input layer and the\n",
    "        last index is the output layer. Everything in between are hidden layers. For example: [4,4,1]\n",
    "    input_units = int <- Takes in the number of binary input units\n",
    "    '''\n",
    "    def __init__(self, network_structure, input_units):\n",
    "        '''\n",
    "        self.network is a 2D-List of perceptrons where the first dimension represents each layer\n",
    "        '''\n",
    "        self.network = []\n",
    "        for layers in network_structure:\n",
    "            layer = []\n",
    "            for point in range(layers):\n",
    "                perceptron = Perceptron(input_units)\n",
    "                layer.append(perceptron)\n",
    "            input_units = len(layer)\n",
    "            self.network.append(layer)\n",
    "        \n",
    "        self.network = np.array(self.network)    \n",
    "        \n",
    "    '''\n",
    "    inputs: np.array([]) <- Takes in the input for each episode. The number of indizes have to be the same as the\n",
    "        number of input_units taken in on __init__. For example: [0,1]\n",
    "    Returns an np.array([]) with the results of this step. In case you have one output-perceptron it may return [1]\n",
    "    '''\n",
    "    def forward_step(self, inputs):\n",
    "        for layer in self.network:\n",
    "            outputs = []\n",
    "            for perceptron in layer:\n",
    "                outputs.append(perceptron.forward_step(inputs))\n",
    "            inputs = np.array(outputs)\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "    '''\n",
    "    output: np.array([]) <- Takes in the actual outputs the MLP generated at the previous forward_step()\n",
    "    target: np.array([]) <- Takes in the expected target values\n",
    "    This function updates all perceptrons using the actual and the target values\n",
    "    '''\n",
    "    def backprop_step(self, output, target):\n",
    "        \n",
    "        deltas = []\n",
    "        \n",
    "        '''\n",
    "        These two for loops calculate the deltas for the output layer and updates all perceptrons in the layer\n",
    "        '''\n",
    "        for j in range(len(self.network[-1])):\n",
    "            deltas.append(-(target[j]-output[j]) * sigmoidprime(self.network[-1][j].drive))\n",
    "        for i in range(len(deltas)):\n",
    "            self.network[-1][i].update(deltas[i])\n",
    "\n",
    "        '''\n",
    "        This for loop calculates all deltas for corresponding hidden layer and updates all perceptrons accordingly\n",
    "        '''\n",
    "        for i in range(len(self.network)-2, -1, -1):\n",
    "            error = 0.0\n",
    "            \n",
    "            for j in range(len(deltas)):\n",
    "                error += (deltas[j] * self.network[i+1][j].weights)\n",
    "            \n",
    "            drives = [k.drive for k in self.network[i]]\n",
    "            new_delta = error * sigmoidprime(np.array(drives))\n",
    "            \n",
    "            for j in range(len(self.network[i])):\n",
    "                self.network[i][j].update(new_delta[j])\n",
    "            deltas = new_delta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 + 6 Training and Visualization\n",
    "\n",
    "### For AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_and\n",
    "mlp = MLP([4,4,1],2)\n",
    "steps = []\n",
    "accuracies = []\n",
    "loss = []\n",
    "for i in range(1000):\n",
    "    steps.append(i)\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "    for k in range(len(possible_inputs)):\n",
    "        sample = possible_inputs[k]\n",
    "        target = t[k]\n",
    "        output = mlp.forward_step(sample)[0]\n",
    "        mlp.backprop_step(np.array([output]), np.array([target]))\n",
    "        accuracy_sum += int(round(output) == target)\n",
    "        loss_sum += pow((target - output), 2)\n",
    "    accuracies.append(accuracy_sum/4)\n",
    "    loss.append(loss_sum/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, loss)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_or\n",
    "mlp = MLP([4,4,1],2)\n",
    "steps = []\n",
    "accuracies = []\n",
    "loss = []\n",
    "for i in range(1000):\n",
    "    steps.append(i)\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "    for k in range(len(possible_inputs)):\n",
    "        sample = possible_inputs[k]\n",
    "        target = t[k]\n",
    "        output = mlp.forward_step(sample)[0]\n",
    "        mlp.backprop_step(np.array([output]), np.array([target]))\n",
    "        accuracy_sum += int(round(output) == target)\n",
    "        loss_sum += pow((target - output), 2)\n",
    "    accuracies.append(accuracy_sum/4)\n",
    "    loss.append(loss_sum/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, loss)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For NOT AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_nand\n",
    "mlp = MLP([4,4,1],2)\n",
    "steps = []\n",
    "accuracies = []\n",
    "loss = []\n",
    "for i in range(1000):\n",
    "    steps.append(i)\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "    for k in range(len(possible_inputs)):\n",
    "        sample = possible_inputs[k]\n",
    "        target = t[k]\n",
    "        output = mlp.forward_step(sample)[0]\n",
    "        mlp.backprop_step(np.array([output]), np.array([target]))\n",
    "        accuracy_sum += int(round(output) == target)\n",
    "        loss_sum += pow((target - output), 2)\n",
    "    accuracies.append(accuracy_sum/4)\n",
    "    loss.append(loss_sum/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, loss)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For NOT OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_nor\n",
    "mlp = MLP([4,4,1],2)\n",
    "steps = []\n",
    "accuracies = []\n",
    "loss = []\n",
    "for i in range(1000):\n",
    "    steps.append(i)\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "    for k in range(len(possible_inputs)):\n",
    "        sample = possible_inputs[k]\n",
    "        target = t[k]\n",
    "        output = mlp.forward_step(sample)[0]\n",
    "        mlp.backprop_step(np.array([output]), np.array([target]))\n",
    "        accuracy_sum += int(round(output) == target)\n",
    "        loss_sum += pow((target - output), 2)\n",
    "    accuracies.append(accuracy_sum/4)\n",
    "    loss.append(loss_sum/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, loss)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For XOR  gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t_xor\n",
    "mlp = MLP([4,4,1],2)\n",
    "steps = []\n",
    "accuracies = []\n",
    "loss = []\n",
    "for i in range(1000):\n",
    "    steps.append(i)\n",
    "    accuracy_sum = 0\n",
    "    loss_sum = 0\n",
    "    for k in range(len(possible_inputs)):\n",
    "        sample = possible_inputs[k]\n",
    "        target = t[k]\n",
    "        output = mlp.forward_step(sample)[0]\n",
    "        mlp.backprop_step(np.array([output]), np.array([target]))\n",
    "        accuracy_sum += int(round(output) == target)\n",
    "        loss_sum += pow((target - output), 2)\n",
    "    accuracies.append(accuracy_sum/4)\n",
    "    loss.append(loss_sum/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps, loss)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
